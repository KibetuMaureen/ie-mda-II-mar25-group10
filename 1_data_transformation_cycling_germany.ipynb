{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abdcd9f-6dfb-4ab9-ad94-20516a3dc78b",
   "metadata": {},
   "source": [
    "# Cycling in numbers - A Case Study of Cycle Paths in Rhine-Kreis Neuss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f3425-9c5b-410c-93a7-b50d2c77f92e",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c22775-1302-4c83-af31-cd79ba80873a",
   "metadata": {},
   "source": [
    "Five Counting stations have been permanently documenting cycling traffic on central roads since 2016 in Cycle paths in the Rhine-Kreis Neuss. The daily measurement of cycling traffic is done with the help of induction loops laid in the ground. With the permanent collection of data can gain insights on the daily, weekly and annual cycles and on it building long-term cycling developments over several years.\n",
    "\n",
    "More details on the data source here: https://data.europa.eu/data/datasets/eco-counter-data-rhein-kreis-neuss?locale=en\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6997bf82-0510-47f3-8142-98a915bde7b8",
   "metadata": {},
   "source": [
    "# Stage 1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee0794-cac8-4304-aa76-22a6741a5c9b",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to clean and transform the data and prepare it for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cbf684-2c7a-4ddc-a254-684d66c2ebf8",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Setup\n",
    "\n",
    "Since we are going to process data stored from HDFS let's start the service\n",
    "\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Hadoop should be running\n",
    "2. Dataset should be in this directory in hadoop: [hdfs://localhost:9000//g10_datalake/bronze/cycling/](hdfs://localhost:9000//g10_datalake/bronze/cycling/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5043ff4-ce8b-4318-87d0-630c01f293f7",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### 1.1 Search for Spark Installation \n",
    "This step is required as we are working in the virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ee1c0b-cfbc-4564-8dc9-4803dd6f6435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b09b7f9-0168-4476-b3ad-09616c1bb832",
   "metadata": {},
   "source": [
    "Changing pandas max column width property to improve data displaying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7fdfeca-fa18-4a40-90d5-70123496870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1197a-c4dd-46b1-aede-29903cdb1f65",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### 1.3 Create SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b952a21-73cf-4d6f-84ff-8a2024e25209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/13 18:31:50 WARN Utils: Your hostname, osbdet resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/03/13 18:31:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/13 18:31:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"group_10_EDA\")\n",
    "    .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b9157-ee9d-4f3a-9e0d-8442e85361e3",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb986c-e664-459f-9ae9-1277d26117c9",
   "metadata": {},
   "source": [
    "Here we read the data from hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9d72c7-5dd0-4cfa-85b2-ae74fe06f83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+------+------+----------+----------+-------------------+\n",
      "|       Id|              Datum|Anzahl|Status|Channel Id|Zählstelle|        Koordinaten|\n",
      "+---------+-------------------+------+------+----------+----------+-------------------+\n",
      "|100019715|2021-10-09 10:00:00|    17|   raw| 101019715| Meerbusch|51.261012, 6.705018|\n",
      "|100019715|2021-10-09 12:00:00|    46|   raw| 101019715| Meerbusch|51.261012, 6.705018|\n",
      "|100019715|2021-10-09 15:00:00|   108|   raw| 101019715| Meerbusch|51.261012, 6.705018|\n",
      "|100019715|2021-10-09 17:00:00|    65|   raw| 101019715| Meerbusch|51.261012, 6.705018|\n",
      "|100019715|2021-10-09 18:00:00|    26|   raw| 101019715| Meerbusch|51.261012, 6.705018|\n",
      "+---------+-------------------+------+------+----------+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (spark.read.option(\"header\", \"true\")\n",
    "                 .option(\"inferSchema\", \"true\")\n",
    "                 .option(\"sep\", \";\")  # Specify semicolon as the delimiter\n",
    "                 .csv(\"hdfs://localhost:9000//g10_datalake/bronze/cycling/\")\n",
    "                 .cache())\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cef2ed-d85d-4076-90e8-d0517b3db9c9",
   "metadata": {},
   "source": [
    "We will rename the columns to English names of the variables and show the schema of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d67d8382-df02-48a8-bc82-451e38b70d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Channel_Id: integer (nullable = true)\n",
      " |-- Counting_Station: string (nullable = true)\n",
      " |-- Coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumnRenamed(\"Datum\", \"Date\") \\\n",
    "       .withColumnRenamed(\"Anzahl\", \"Count\") \\\n",
    "       .withColumnRenamed(\"Zählstelle\", \"Counting_Station\") \\\n",
    "       .withColumnRenamed(\"Koordinaten\", \"Coordinates\") \\\n",
    "       .withColumnRenamed(\"Channel Id\", \"Channel_Id\")\n",
    "\n",
    "# Show updated schema\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1684241e-d6ca-4fa4-94ea-3e565fd47c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Count</th>\n",
       "      <th>Status</th>\n",
       "      <th>Channel_Id</th>\n",
       "      <th>Counting_Station</th>\n",
       "      <th>Coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100019715</td>\n",
       "      <td>2021-10-09 10:00:00</td>\n",
       "      <td>17</td>\n",
       "      <td>raw</td>\n",
       "      <td>101019715</td>\n",
       "      <td>Meerbusch</td>\n",
       "      <td>51.261012, 6.705018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100019715</td>\n",
       "      <td>2021-10-09 12:00:00</td>\n",
       "      <td>46</td>\n",
       "      <td>raw</td>\n",
       "      <td>101019715</td>\n",
       "      <td>Meerbusch</td>\n",
       "      <td>51.261012, 6.705018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100019715</td>\n",
       "      <td>2021-10-09 15:00:00</td>\n",
       "      <td>108</td>\n",
       "      <td>raw</td>\n",
       "      <td>101019715</td>\n",
       "      <td>Meerbusch</td>\n",
       "      <td>51.261012, 6.705018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100019715</td>\n",
       "      <td>2021-10-09 17:00:00</td>\n",
       "      <td>65</td>\n",
       "      <td>raw</td>\n",
       "      <td>101019715</td>\n",
       "      <td>Meerbusch</td>\n",
       "      <td>51.261012, 6.705018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100019715</td>\n",
       "      <td>2021-10-09 18:00:00</td>\n",
       "      <td>26</td>\n",
       "      <td>raw</td>\n",
       "      <td>101019715</td>\n",
       "      <td>Meerbusch</td>\n",
       "      <td>51.261012, 6.705018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id                Date  Count Status  Channel_Id Counting_Station  \\\n",
       "0  100019715 2021-10-09 10:00:00     17    raw   101019715        Meerbusch   \n",
       "1  100019715 2021-10-09 12:00:00     46    raw   101019715        Meerbusch   \n",
       "2  100019715 2021-10-09 15:00:00    108    raw   101019715        Meerbusch   \n",
       "3  100019715 2021-10-09 17:00:00     65    raw   101019715        Meerbusch   \n",
       "4  100019715 2021-10-09 18:00:00     26    raw   101019715        Meerbusch   \n",
       "\n",
       "           Coordinates  \n",
       "0  51.261012, 6.705018  \n",
       "1  51.261012, 6.705018  \n",
       "2  51.261012, 6.705018  \n",
       "3  51.261012, 6.705018  \n",
       "4  51.261012, 6.705018  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85266a-4c58-41d4-8a29-e668d947c4f3",
   "metadata": {},
   "source": [
    "**Summary Statistics of the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8feb650-8e12-4ad4-b91f-36c97a90ef34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===========================================================(2 + 0) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+------------------+---------+---------+-------------+\n",
      "|Counting_Station|         avg_count|      stddev_count|min_count|max_count|count_entries|\n",
      "+----------------+------------------+------------------+---------+---------+-------------+\n",
      "|           Neuss| 4.195613401689653| 9.646175144591327|        0|      211|       276980|\n",
      "|       Meerbusch| 6.108348822542054|12.980603314519595|        0|      234|       299671|\n",
      "|        Dormagen|0.8086672225972548| 2.517983888602218|        0|       85|       453294|\n",
      "|          Jüchen| 2.273837380328395|7.3598334275448645|        0|     1855|       299152|\n",
      "|    Grevenbroich|1.3672415745891118|3.7640586442326884|        0|      103|       317848|\n",
      "+----------------+------------------+------------------+---------+---------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, stddev, min, max, count\n",
    "\n",
    "summary_stats = df.groupBy(\"Counting_Station\").agg(\n",
    "    mean(\"Count\").alias(\"avg_count\"),\n",
    "    stddev(\"Count\").alias(\"stddev_count\"),\n",
    "    min(\"Count\").alias(\"min_count\"),\n",
    "    max(\"Count\").alias(\"max_count\"),\n",
    "    count(\"Count\").alias(\"count_entries\")\n",
    ")\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975467e5-e544-43c5-aa76-7d96139cce97",
   "metadata": {},
   "source": [
    "**Adding Date information to the dataset**\n",
    "\n",
    "From the date column, we will extract the Year, Month, Day, Hour and Day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1f9cd6-073d-4c1c-8351-64b92ecb0b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----+------+----------+----------------+-------------------+----+-----+---+----+-----------+\n",
      "|       Id|               Date|Count|Status|Channel_Id|Counting_Station|        Coordinates|Year|Month|Day|Hour|Day_of_Week|\n",
      "+---------+-------------------+-----+------+----------+----------------+-------------------+----+-----+---+----+-----------+\n",
      "|100019715|2021-10-09 10:00:00|   17|   raw| 101019715|       Meerbusch|51.261012, 6.705018|2021|   10|  9|  10|          7|\n",
      "|100019715|2021-10-09 12:00:00|   46|   raw| 101019715|       Meerbusch|51.261012, 6.705018|2021|   10|  9|  12|          7|\n",
      "|100019715|2021-10-09 15:00:00|  108|   raw| 101019715|       Meerbusch|51.261012, 6.705018|2021|   10|  9|  15|          7|\n",
      "|100019715|2021-10-09 17:00:00|   65|   raw| 101019715|       Meerbusch|51.261012, 6.705018|2021|   10|  9|  17|          7|\n",
      "|100019715|2021-10-09 18:00:00|   26|   raw| 101019715|       Meerbusch|51.261012, 6.705018|2021|   10|  9|  18|          7|\n",
      "+---------+-------------------+-----+------+----------+----------------+-------------------+----+-----+---+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, hour, dayofweek\n",
    "\n",
    "df = df.withColumn(\"Year\", year(col(\"Date\"))) \\\n",
    "       .withColumn(\"Month\", month(col(\"Date\"))) \\\n",
    "       .withColumn(\"Day\", dayofmonth(col(\"Date\"))) \\\n",
    "       .withColumn(\"Hour\", hour(col(\"Date\"))) \\\n",
    "       .withColumn(\"Day_of_Week\", dayofweek(col(\"Date\")))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab2cd8e-f3b5-43e5-a779-8d6e30effe3d",
   "metadata": {},
   "source": [
    "**Handling Location information**\n",
    "\n",
    "We need to split the latitude and longitude information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "382bb15b-a0d7-4e9c-9fac-7e9590605de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----+------+----------+----------------+-------------------+----+-----+---+----+-----------+---------+---------+\n",
      "|       Id|               Date|Count|Status|Channel_Id|Counting_Station|        Coordinates|Year|Month|Day|Hour|Day_of_Week| Latitude|Longitude|\n",
      "+---------+-------------------+-----+------+----------+----------------+-------------------+----+-----+---+----+-----------+---------+---------+\n",
      "|100019715|2021-10-09 10:00:00|   17|   raw| 101019715|       Meerbusch|51.261012, 6.705018|2021|   10|  9|  10|          7|51.261012| 6.705018|\n",
      "|100019715|2021-10-09 12:00:00|   46|   raw| 101019715|       Meerbusch|51.261012, 6.705018|2021|   10|  9|  12|          7|51.261012| 6.705018|\n",
      "|100019715|2021-10-09 15:00:00|  108|   raw| 101019715|       Meerbusch|51.261012, 6.705018|2021|   10|  9|  15|          7|51.261012| 6.705018|\n",
      "|100019715|2021-10-09 17:00:00|   65|   raw| 101019715|       Meerbusch|51.261012, 6.705018|2021|   10|  9|  17|          7|51.261012| 6.705018|\n",
      "|100019715|2021-10-09 18:00:00|   26|   raw| 101019715|       Meerbusch|51.261012, 6.705018|2021|   10|  9|  18|          7|51.261012| 6.705018|\n",
      "+---------+-------------------+-----+------+----------+----------------+-------------------+----+-----+---+----+-----------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df = df.withColumn(\"Latitude\", split(col(\"Coordinates\"), \", \")[0]) \\\n",
    "       .withColumn(\"Longitude\", split(col(\"Coordinates\"), \", \")[1])\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091f885e-8f71-46b4-858f-e8756411b530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Channel_Id: integer (nullable = true)\n",
      " |-- Counting_Station: string (nullable = true)\n",
      " |-- Coordinates: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Day_of_Week: integer (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show updated schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4dfe0-fbfa-4d5e-b164-c4fd5f240bb6",
   "metadata": {},
   "source": [
    "**Write the data back to hadoop**\n",
    "\n",
    "We will write the transformed data to hadoop. We will also get the data from individual counting statons and write them as separate files to hadoop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1018b4a0-3f45-4c6b-9703-b4520fef75e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 323984 rows for Counting_Station Neuss to hdfs://localhost:9000/g10_datalake/silver/cycling/Counting_Station_Neuss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 319680 rows for Counting_Station Meerbusch to hdfs://localhost:9000/g10_datalake/silver/cycling/Counting_Station_Meerbusch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 488430 rows for Counting_Station Dormagen to hdfs://localhost:9000/g10_datalake/silver/cycling/Counting_Station_Dormagen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 314568 rows for Counting_Station Jüchen to hdfs://localhost:9000/g10_datalake/silver/cycling/Counting_Station_Jüchen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 324384 rows for Counting_Station Grevenbroich to hdfs://localhost:9000/g10_datalake/silver/cycling/Counting_Station_Grevenbroich\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get unique Counting_Station values\n",
    "unique_stations = df.select(\"Counting_Station\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Define output path in HDFS\n",
    "output_base_path = \"hdfs://localhost:9000/g10_datalake/silver/cycling/\"\n",
    "\n",
    "# Split and save DataFrames\n",
    "for index, station in enumerate(unique_stations, start=1):\n",
    "    # Filter data for each Counting_Station\n",
    "    subset_df = df.filter(df.Counting_Station == station)\n",
    "    \n",
    "    # Define output path\n",
    "    output_path = f\"{output_base_path}Counting_Station_{station}\"\n",
    "    \n",
    "    # Save CSV\n",
    "    subset_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "\n",
    "    # Display row count for each split\n",
    "    print(f\"Saved {subset_df.count()} rows for Counting_Station {station} to {output_path}\")\n",
    "\n",
    "\n",
    "output_path = f\"{output_base_path}cleaned_cycling_data\"\n",
    "df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae011657-2d15-44a8-a158-1846b132ed24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1207c76-055c-4509-9982-86b7ac60b589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
